Code and Data corresponding to [Explaining and Generalizing Skip-Gram through Exponential Family Principal Component Analysis](https://ryancotterell.github.io/papers/cotterell+alb.eacl17.pdf)

## Example/Instructions
To run the tensor-factoization word embeddings code, run `bash make-embeddings.sh` from the `hyperref-tensor` directory
<br>Right now, that example uses an early paragraph from Thoreau's *Walden* extracted from this [website](https://textfiles.com/etext/NONFICTION/thoreau-walden-186.txt)



##Directories: 
1. ```hyperref-tensor```: C code to generate word embeddings. The directory includes the original word2vecf code which we use as a baseline.

1. ```Universal_Dependencies```: Data and code corresponding to UD experiments in the paper 
